##  æ¨¡å‹ç»“æ„

- **architectures**: `"Qwen3ForCausalLM"`
   è¡¨æ˜è¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆCausal LMï¼‰çš„ Qwen3 æ¨¡å‹ã€‚
- **model_type**: `"qwen3"`
   æŒ‡å®šæ¨¡å‹ç±»åˆ«ï¼Œå’Œ Hugging Face Transformers æ¡†æ¶å…¼å®¹ã€‚
- **hidden_size**: `1024`
   æ¯å±‚ Transformer çš„éšè—ç»´åº¦å¤§å°ã€‚
- **intermediate_size**: `3072`
   å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰çš„ä¸­é—´å±‚ç»´åº¦ï¼Œé€šå¸¸æ˜¯ `hidden_size` çš„ 3 å€ã€‚
- **num_hidden_layers**: `28`
   æ¨¡å‹æœ‰ 28 å±‚ Transformer blockã€‚
- **num_attention_heads**: `16`
   æ¯å±‚æœ‰ 16 ä¸ªæ³¨æ„åŠ›å¤´ã€‚
- **head_dim**: `128`
   æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œæ‰€ä»¥æ³¨æ„åŠ›æ€»ç»´åº¦ = 16 Ã— 128 = 2048ï¼ˆè¿™é‡Œé€šå¸¸ä¼šç­‰äº hidden_sizeï¼Œä½† Qwen3 å¯èƒ½ç”¨äº†æŠ•å½±ï¼‰ã€‚
- **num_key_value_heads**: `8`
   é‡‡ç”¨äº† **åˆ†ç»„ KV Cache**ï¼ˆGrouped Query Attention, GQAï¼‰ï¼Œå³ 16 query å¤´å…±äº« 8 ä¸ª key/value å¤´ï¼Œé™ä½æ¨ç†æ˜¾å­˜å ç”¨ã€‚

------

## ğŸ§  æ³¨æ„åŠ›ä¸ä½ç½®ç¼–ç 

- **attention_bias**: `false`
   ä¸ä½¿ç”¨ attention biasã€‚
- **attention_dropout**: `0.0`
   æ³¨æ„åŠ›å±‚ä¸åš dropoutï¼Œä¿è¯æ¨ç†ç¨³å®šæ€§ã€‚
- **max_position_embeddings**: `40960`
   æ”¯æŒæœ€é•¿ 40,960 token çš„åºåˆ—ã€‚
- **rope_theta**: `1000000`
   æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰çš„åŸºæ•°ï¼Œå€¼è¶Šå¤§ä»£è¡¨æ›´é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚
- **rope_scaling**: `null`
   æ²¡æœ‰é¢å¤–çš„ RoPE scalingï¼Œç›´æ¥ä½¿ç”¨ä¸Šé¢çš„ Î¸ã€‚
- **sliding_window**: `null`
   **use_sliding_window**: `false`
   æ²¡æœ‰é‡‡ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆå³å…¨ä¸Šä¸‹æ–‡å¯è§ï¼‰ã€‚
- **max_window_layers**: `28`
   è¡¨æ˜æ‰€æœ‰ 28 å±‚éƒ½æ”¯æŒé•¿ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½ç”¨äºé…ç½®åˆ†å±‚çª—å£æ³¨æ„åŠ›æ—¶ç”Ÿæ•ˆï¼‰ã€‚

------

## ğŸ”¢ å½’ä¸€åŒ–ä¸æ¿€æ´»

- **hidden_act**: `"silu"`
   å‰é¦ˆç½‘ç»œæ¿€æ´»å‡½æ•°ä¸º SiLU (Swish)ã€‚
- **rms_norm_eps**: `1e-06`
   RMSNorm çš„æ•°å€¼ç¨³å®šæ€§å‚æ•° Îµã€‚

------

## ğŸ§© è¯è¡¨ä¸åµŒå…¥

- **vocab_size**: `151936`
   è¯è¡¨å¤§å°ï¼Œæ¥è¿‘ 15 ä¸‡ã€‚
- **tie_word_embeddings**: `true`
   è¾“å…¥å’Œè¾“å‡ºè¯åµŒå…¥æƒé‡å…±äº«ï¼Œå‡å°‘å‚æ•°é‡ã€‚

------

## âš™ï¸ è®­ç»ƒä¸æ¨ç†ç›¸å…³

- **initializer_range**: `0.02`
   å‚æ•°åˆå§‹åŒ–èŒƒå›´ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ã€‚
- **torch_dtype**: `"bfloat16"`
   è®­ç»ƒ/æ¨ç†æ—¶ä½¿ç”¨ bfloat16 ç²¾åº¦ã€‚
- **use_cache**: `true`
   ç”Ÿæˆæ—¶ä½¿ç”¨ KV ç¼“å­˜ï¼ŒåŠ é€Ÿæ¨ç†ã€‚
- **transformers_version**: `"4.51.0"`

[//]: # (   è¡¨æ˜è¿™æ˜¯åœ¨ Hugging Face Transformers 4.51.0 ç‰ˆæœ¬ä¸‹å¯¼å‡ºçš„é…ç½®ã€‚)

-------

```shell
Qwen3Model(
  (embed_tokens): Embedding(151936, 1024)
  (layers): ModuleList(
    (0-27): 28 x Qwen3DecoderLayer(
      (self_attn): Qwen3Attention(
        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
      )
      (mlp): Qwen3MLP(
        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
    )
  )
  (norm): Qwen3RMSNorm((1024,), eps=1e-06)
  (rotary_emb): Qwen3RotaryEmbedding()
)
```
---
## ğŸ” æ¨¡å—è§£è¯»

1. **åµŒå…¥å±‚**

   ```
   (embed_tokens): Embedding(151936, 1024)
   ```

   - è¯è¡¨å¤§å° = 151,936
   - ç»´åº¦ = 1024
   - å‚æ•°é‡ = `151936 Ã— 1024 â‰ˆ 155M`

2. **Decoder å±‚ï¼ˆå…± 28 å±‚ï¼‰**
    æ¯ä¸€å±‚åŒ…å«ï¼š

   - **Self-Attention**
     - q_proj: `1024 â†’ 2048`
     - k_proj: `1024 â†’ 1024`
     - v_proj: `1024 â†’ 1024`
     - o_proj: `2048 â†’ 1024`
        â‡’ å‚æ•°é‡å¤§çº¦ `2.048M + 1.048M + 1.048M + 2.097M â‰ˆ 6.2M`
   - **MLP**
     - gate_proj: `1024 â†’ 3072`
     - up_proj: `1024 â†’ 3072`
     - down_proj: `3072 â†’ 1024`
        â‡’ å‚æ•°é‡å¤§çº¦ `3.145M + 3.145M + 3.145M â‰ˆ 9.4M`
   - **RMSNorm**
      åŸºæœ¬ä¸Šæ˜¯ `O(hidden_size)` çš„å‚æ•°é‡ï¼ˆ1k é‡çº§ï¼Œå¯ä»¥å¿½ç•¥ä¸è®¡ï¼‰ã€‚

   ğŸ”¹ æ‰€ä»¥å•å±‚å‚æ•° â‰ˆ `6.2M + 9.4M â‰ˆ 15.6M`

   ğŸ”¹ 28 å±‚æ€»å‚æ•° â‰ˆ `28 Ã— 15.6M â‰ˆ 436M`

3. **æœ€ç»ˆ LayerNorm**

   ```
   (norm): Qwen3RMSNorm((1024,))
   ```

   - åªæœ‰ 1024 ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå¯å¿½ç•¥ã€‚

------

## ğŸ“Š æ€»å‚æ•°ä¼°ç®—

- **Embedding**: ~155M
- **28 å±‚ Decoder**: ~436M
- **è¾“å‡ºå±‚**ï¼ˆtie embedding â†’ å…±äº«è¾“å…¥åµŒå…¥æƒé‡ï¼Œä¸å•ç‹¬å¢åŠ å‚æ•°ï¼‰

ğŸ‘‰ æ€»è®¡å¤§çº¦ **591M å‚æ•°**ï¼Œä¹Ÿå°±æ˜¯ **6 äº¿è§„æ¨¡æ¨¡å‹**ã€‚

------

## âœ… æ€»ç»“

è¿™ä»½ Qwen3 æ¨¡å‹å¤§è‡´è§„æ¨¡ï¼š

- **28 å±‚ï¼ŒHidden size = 1024ï¼ŒFFN = 3072ï¼Œ16 å¤´æ³¨æ„åŠ›ï¼ˆGQAï¼‰**
- **æ€»å‚æ•°é‡ â‰ˆ 6 äº¿**

---

## ğŸ”¹ åŸºæœ¬è®¾å®š (Qwen3 é…ç½®)

- `hidden_size = 1024`
- `head_dim = 128`
- `num_attention_heads = 16` ï¼ˆQ å¤´æ•°ï¼‰
- `num_key_value_heads = 8` ï¼ˆKV å¤´æ•°ï¼‰

æ‰€ä»¥ï¼š

- Q: `[B, L, 2048] â†’ reshape â†’ [B, L, 16, 128]`
- K: `[B, L, 1024] â†’ reshape â†’ [B, L, 8, 128]`
- V: `[B, L, 1024] â†’ reshape â†’ [B, L, 8, 128]`

------

## ğŸ”¹ Attention çŸ©é˜µè®¡ç®—

1. **æ ‡å‡†å¤šå¤´æ³¨æ„åŠ› (å¯¹ç§°æƒ…å†µ)**
   $$
   \text{scores} = Q \cdot K^T
   $$

   - Q: `[B, 16, L, 128]`
   - K: `[B, 16, L, 128]`
   - scores: `[B, 16, L, L]`

2. **GQA çš„æƒ…å†µ (Q å¤šï¼ŒKV å°‘)**

   - Q: `[B, 16, L, 128]`
   - K: `[B, 8, L, 128]`
   - **æ˜ å°„è§„åˆ™**ï¼š16 ä¸ª Q å¤´æ˜ å°„åˆ° 8 ä¸ª KV å¤´ï¼ˆä¸¤ä¸ª Q å¤´å…±äº«ä¸€ä¸ª KV ç»„ï¼‰
     - ä¾‹å¦‚ Q[0], Q[1] éƒ½ç”¨ K[0]
     - Q[2], Q[3] éƒ½ç”¨ K[1]
     - â€¦ä»¥æ­¤ç±»æ¨
   - scores: `[B, 16, L, L]` ï¼ˆå› ä¸ºæ¯ä¸ª Q head éƒ½èƒ½å’Œå¯¹åº”çš„ K ç»„åšä¹˜ç§¯ï¼‰

3. **Attention åº”ç”¨åˆ° V**

   - Softmax(scores): `[B, 16, L, L]`
   - V: `[B, 8, L, 128]`
   - æ˜ å°„åŒæ ·å…±äº«è§„åˆ™ï¼ˆ2 ä¸ª Q å¤´å¯¹åº”åŒä¸€ä¸ª V å¤´ï¼‰
   - è¾“å‡º: `[B, 16, L, 128]`

4. **æ‹¼æ¥ + è¾“å‡ºæŠ•å½±**

   - æ‹¼æ¥ â†’ `[B, L, 16 Ã— 128] = [B, L, 2048]`
   - o_proj â†’ `[B, L, 1024]`

------

## âœ… æ€»ç»“ï¼šAttention çŸ©é˜µç»´åº¦

- **scores (QK^T)**: `[B, 16, L, L]`
- **attention Ã— V**: `[B, 16, L, 128]`
- **æ‹¼æ¥å**: `[B, L, 2048]`
- **æŠ•å½±å› hidden_size**: `[B, L, 1024]`